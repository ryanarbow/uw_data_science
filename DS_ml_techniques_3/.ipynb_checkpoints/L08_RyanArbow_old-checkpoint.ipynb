{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson 8\n",
    "\n",
    "For this assignment you will start from the perceptron neural network notebook (Simple Perceptron Neural Network.ipynb) and modify the python code to make it into a multi-layer neural network classifier. To test your system, use the RedWhiteWine.csv file with the goal of building a red or white wine classifier. Use all the features in the dataset, allowing the network to decide how to build the internal weighting system. To review the data attributes, download the L08_WineQuality.pdf.  Perform each of the following tasks and answer the related questions:\n",
    "\n",
    "- Use the provided RedWhiteWine.csv file. Include ALL the features with “Class” being your output vector\n",
    "- Use the provided Simple Perceptron Neural Network notebook to develop a multi-layer feed-forward/backpropagation neural network\n",
    "- Be able to adjust the following between experiments:\n",
    "    - Learning Rate\n",
    "    - Number of epochs\n",
    "    - Depth of architecture—number of hidden layers between the input and output layers\n",
    "    - Number of nodes in a hidden layer—width of the hidden layers\n",
    "    - (optional) Momentum\n",
    "- Determine what the best neural network structure and hyperparameter settings results in the best predictive capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Create an artificial dataset\n",
    "# x1 = np.array(np.arange(0.1,0.7,0.1))\n",
    "# X1 = np.exp(x1 * 1.1 + 0.75)\n",
    "# x2 = np.array(np.arange(0.6,1.2,0.1))\n",
    "# X2 = np.exp(x2 * 0.4 + 0.75)\n",
    "\n",
    "# #From the output, lets use 3 as threshold; Value>3 = class 1, value<3 = class 0\n",
    "# X = np.array([X1,X2])\n",
    "# Y = np.array([0,0,0,1,1,1])\n",
    "\n",
    "# print(X)\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('RedWhiteWine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predictors (X) and target (Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:,0:11]\n",
    "Y = data.iloc[:,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic (Sigmoid) Function\n",
    "Exponential values for moderately large numbers tend to overflow. So np.clip is used here to limit the values of the signal between -500 and 500. Since e^x is between 0 and 1, the error in using this clip is low. Additionally, I am using logistic (sigmoid) function $\\frac{1}{1+e^-z}$. This can also be expressed as $\\frac{e^z}{1+e^z}$. NOTE: you could call sklearn.linear_model.LogisticRegressionCV(), but it's always good to try and write it yourself so you understand what the function does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a numerically stable logistic s-shaped definition to call\n",
    "def sigmoid(x):\n",
    "    x = np.clip(x, -500, 500)\n",
    "    if x.any()>=0:\n",
    "        return 1/(1 + np.exp(-x))\n",
    "    else:\n",
    "        return np.exp(x)/(1 + np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Parameters\n",
    "Because this there are not hidden layers, the second dimension is always assigned to 1. std is set to ${1^{-1}}$ to ensure values are between zero and 1. If zeros, there's no reason to multiply with std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dimensions and set the weights to random numbers\n",
    "def init_parameters(dim1, dim2, dim3=1,std=1e-1, random = True):\n",
    "    if(random):\n",
    "        return(np.random.random([dim1,dim2,dim3])*std)\n",
    "    else:\n",
    "        return(np.zeros([dim1,dim2,dim3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "Here, I am assuming a single layered network. Note that event with single layered network, the layer itself can have multiple nodes. Also, I am using vectorized operations here i.e not using explicit loops. This helps in processing multiple inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single layer network: Forward Prop\n",
    "# Passed in the weight vectors, bias vector, the input vector and the Y\n",
    "def fwd_prop(W1,bias,X,Y):\n",
    "\n",
    "    Z1 = np.dot(W1,X) + bias # dot product of the weights and X + bias\n",
    "    A1 = sigmoid(Z1)  # Uses sigmoid to create a predicted vector\n",
    "\n",
    "    return(A1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am calculating the loss/cost. The loss function here is a logistic loss function and in this case of binary classification, this is also a cross-entropy loss\n",
    "\n",
    "Cross Entropy loss for a single datapoint = $\\sum_{i=1}^{c} y_i*log (\\hat y_i) $\n",
    "For binary classification: $y_i*log (\\hat y_i) + (1-y_i)*log(1-\\hat y_i) $\n",
    "\n",
    "Lastly, the gradients W1 and B1 are calculated and returned along with the total cost/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Single layer network: Backprop\n",
    "\n",
    "def back_prop(A1,W1,bias,X,Y):\n",
    "\n",
    "    m = np.shape(X)[1] # used the calculate the cost by the number of inputs -1/m\n",
    "   \n",
    "    # Cross entropy loss function\n",
    "    cost = (-1/m)*np.sum(Y*np.log(A1) + (1-Y)*np.log(1-A1)) # cost of error\n",
    "    dZ1 = A1 - Y                                            # subtract actual from pred weights\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)                          # calc new weight vector\n",
    "    dBias = (1/m) * np.sum(dZ1, axis = 1, keepdims = True)  # calc new bias vector\n",
    "    \n",
    "    grads ={\"dW1\": dW1, \"dB1\":dBias} # Weight and bias vectors after backprop\n",
    "    \n",
    "    return(grads,cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "This function performs a simple gradient descent. After completing a round of forward propagation and backward propagation, the weights are updated based on the learning rate and gradient. The loss for that iteration is recorded in the loss_array. The function returns the final parameters W1 (updated weight vector), B1 (bias) and the loss array after running for given number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grad_desc(num_epochs,learning_rate,X,Y,n_1):\n",
    "    \n",
    "    n_0, m = np.shape(X)\n",
    "    \n",
    "    W1 = init_parameters(n_1, n_0, True)\n",
    "    B1 = init_parameters(n_1,1, True)\n",
    "    \n",
    "    loss_array = np.ones([num_epochs])*np.nan # resets the loss_array to NaNs\n",
    "    \n",
    "    for i in np.arange(num_epochs):\n",
    "        A1 = fwd_prop(W1,B1,X,Y)                # get predicted vector\n",
    "        grads,cost = back_prop(A1,W1,B1,X,Y)    # get gradient and the cost from BP \n",
    "        \n",
    "        W1 = W1 - learning_rate*grads[\"dW1\"]    # update weight vector LR*gradient*[BP weights]\n",
    "        B1 = B1 - learning_rate*grads[\"dB1\"]    # update bias LR*gradient[BP bias]\n",
    "        \n",
    "        loss_array[i] = cost                    # loss array gets cross ent values\n",
    "        \n",
    "        parameter = {\"W1\":W1,\"B1\":B1}           # assign \n",
    "    \n",
    "    return(parameter,loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Experiment\n",
    "Now that all of the helper functions are created we can run gradient descent on the handcrafted dataset that I had created earlier. Note that I am using n_1 = 1, therefore, I am just using one output node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "params, loss_array = run_grad_desc(num_epochs,learning_rate,X,Y,n_1=3)\n",
    "#print(loss_array[num_epochs-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the boundary of separation is 0. That is values less than 0 belong to class 0 and greater than 0 belong to class 1.\n",
    "Key thing to note here is that the data we generated was a linearly separable data and hence there are many possible options for the separting plane. Unlike SVM, logistic regression does not necessarily find the best separting plane, but finds a locally optimum solution that separates the classes of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot of the loss array\n",
    "Here we want to ensure that the loss value per iteration is going down. However, note that the plot has not curved to reach stablizing value i.e we can run the algorithms more times to get a lower loss. However, this is not needed as the current value of the parameters can classify the input data accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) #Set default plot sizes\n",
    "plt.rcParams['image.interpolation'] = 'nearest' #Use nearest neighbor for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example with handcrafted dataset\n",
    "values below 0.5 are assigned to class 1 and values above are set to class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = np.array([[0.25,0.75],[0.1,0.9],[0.3,0.8],[0.8,0.25],[0.9,0.2],[0.7,0.1]])\n",
    "X = np.array([[0.11,0.12],[0.05,0.1],[0.15,0.11],[0.8,0.9],[0.9,0.8],[0.85,0.95]])\n",
    "X = X.T #Had to do this because, I did not declare the X array as (#dimension * # Datapoints)\n",
    "Y = np.array([1,1,1,0,0,0])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params, loss_array = run_grad_desc(100000,0.01,X,Y,n_1= 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to Consider\n",
    "How would you stop this perceptron from overfitting?\n",
    "How would you convert this to a multi-layer Neural Network for a complex problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
