{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>3 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 - Introduction to Naive Bayes Models\n",
    "\n",
    "This notebook introduces you to naive Bayes models. Naive Bayes models are a surprisingly useful and effective simplification of the general Bayesian models. Naive Bayes models make the naive assumption of independence of the features.\n",
    "\n",
    "Some properties of naive Bayes models are:\n",
    "\n",
    "- Do not require a prior\n",
    "- Computational complexity is linear in number of parameter/features\n",
    "- Require minimal data to produce models that generalizes well\n",
    "- Have a simple and inherent regularization\n",
    "\n",
    "Naive Bayes models are widely used including for:\n",
    "\n",
    "- Document classification\n",
    "- SPAM detection\n",
    "- Image classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Naive Bayes Models\n",
    "\n",
    "Recall Bayes theorem:\n",
    "\n",
    "$$P(A|B) = P(B|A) \\frac{P(A)}{P(B)}$$\n",
    "\n",
    "Using the chain rule of conditional probabilities, we can create write the joint distribution for the probability of class $C_k$ as: \n",
    "\n",
    "$$\n",
    "p(C_k, x_1, x_2, x_3, \\ldots, x_n) = p(x_1, x_2, x_3, \\ldots, x_n, C_k) \\\\\n",
    "=  p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2, x_3, \\ldots, x_n, C_k) \\\\\n",
    "= p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2 | x_3, \\ldots, x_n, C_k) p(x_3, \\ldots, x_n, C_k) \\\\\n",
    "\\cdots \\cdots \\\\\\\n",
    "=  p(x_1 | x_2, x_3, \\ldots, x_n, C_k)  p(x_2 | x_3, \\ldots, x_n, C_k) \\dots p(C_k)\n",
    "$$\n",
    "\n",
    "Let the features, $\\{ x_1, x_2, x_3, \\ldots, x_n \\}$, be independent, the above can then be written as:\n",
    "\n",
    "$$p(x_i | x_{i + 1}, x_{i + 2}, \\ldots, x_n, C_k) = p(x_i | C_k)$$\n",
    "\n",
    "This simpification allows us to write the probability of the class $C_k$ as the conditional distribution:\n",
    "\n",
    "$$p(C_k | x_1, x_2, x_3, \\ldots, x_n) \\propto p(C_k) \\prod^N_{j = 1} p(x_j|C_k)$$\n",
    "\n",
    "Given a number of classes, we can find the mostly likely class $\\hat{y}$ as:\n",
    "\n",
    "$$\\hat{y} = argmax_k \\Big[ \\prod^N_{j = 1} p(x_j|C_k) \\Big]$$\n",
    "\n",
    "Notice that the above formulation uses only the empirical probabilities of the features conditioned on the class. Further no prior distribution is required. \n",
    "\n",
    "### Pitfalls in Naive Bayes Models\n",
    "\n",
    "There are some well known pitfalls with known solutions, including:\n",
    "\n",
    "- Multiplication of small probabilities leads to floating point underflow. This problem is corrected by computing with the log probabilities, $ln(p)$.\n",
    "- If there are no samples/data then $p(x_j|C_k) = 0$, leading the product of probabilities to be 0. A Laplace smoother is used to ensure that all $p(x_.j|C_k) > 0$\n",
    "- Collinear features do not exhibit independence. Ideally, such features should be removed from the the data set to prevent problems with the  model.\n",
    "- Regularization is generally a minor issue with naive Bayes models, as uninformative features tend to a uniform distribution which does not affect the outcome.\n",
    "\n",
    "### Types of Naive Bayes Models\n",
    "\n",
    "Now that we have looked into the basics of a naive Bayes models, let's look at some specific formulations. It is important to keep in mind that a specific naive Bayes model is required for each class of problem.  \n",
    "\n",
    "The **multinomial naive Bayes classifier** is a widely used form of the model. The multinomial classifier finds the mostly likely class from multiple possibilities. To prevent numerical underflow we write this classifier taking the logarithms of both sides of the equation as follows:\n",
    "\n",
    "$$log \\Big( p(C_k | x) \\Big) \\propto\\ log \\Big( p(C_k) \\prod^N_{j = 1} p_{kj}(x_i) \\Big)\\\\\n",
    "= log \\Big( p(C_k) \\Big) + log \\Big( \\sum^N_{j = 1} p_{kj}(x_i) \\Big)$$\n",
    "\n",
    "The most likely class $\\hat{y}$ is then:\n",
    "\n",
    "$$\\hat{y} = argmax_k \\Big[ log \\Big( p(C_k) \\Big) + log \\Big( \\sum^N_{j = 1} p_{kj}(x_i) \\Big) \\Big]$$\n",
    "\n",
    "The multinomial classifier can be simplified for the Bernoulli or binary case as:\n",
    "\n",
    "$$log \\Big( p(x | C_k) \\Big) = log \\Big( \\sum^N_{j = 1} p_{kj}^{x_i} \\big( 1 -  p_{kj}^{(1 - x_i)} \\big) \\Big)$$\n",
    "\n",
    "\n",
    "### Document Classification with Naive Bayes\n",
    "\n",
    "Document classification has been one of the most successful applications of the naive Bayes model. There is a good chance that the SPAM filter your email service uses is a naive Bayes model, at least in part. \n",
    "\n",
    "\n",
    "We say that we classify documents by **topics**. The naive Bayes topic model computes the probability that a document $D$ has topic $C$ based on the occurrence of the words $\\{ w_1, w_2, \\ldots, w_n \\}$, using the following relationship:\n",
    "\n",
    "$$p(C|D) \\propto \\prod_{j = 1}^N p(w_j|C)$$\n",
    "\n",
    "Notice that this topic model allows a document to have a number of topics. For example, we can say the topics of $D$ are the 5 topics with the highest probability.\n",
    "\n",
    "For a SPAM classifier, we only need a Bernoulli topic model:\n",
    "\n",
    "$$p(S+|D) \\propto p(S+) \\prod_{j=1}^N p(w_j|S+)$$\n",
    "\n",
    "An hypothesis test is applied to each message to determine if it is SPAM. We use the log likelihood ratio to determine if a given message is SPAM or not. If the following ratio is $> 1$ we classify the message as SPAM:\n",
    "\n",
    "$$ln \\Bigg( \\frac{p(S+)}{p(S-)} \\Bigg) = \\sum_{j = 1}^N \\Big[ log \\big( p(w_j|S+) \\big) - log \\big( p(w_j|S-) \\big) \\Big]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Naive Bayes Model - House Votes\n",
    "\n",
    "Let's try a simple example of a naive bayes model. The R mlbench package contains the `HouseVotes84` which contains political party and votes on 16 important bills for 435 members of the US House of Representatives in 1984. We will use this data set to build and test a classifier to predict the political party of the congresspeople. \n",
    "\n",
    "Execute the code in the cell below to load the data and examine the dimensions and head of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://archive.ics.uci.edu/ml/datasets/congressional+voting+records\n",
    "votes = pandas.read_csv('https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/house-votes-84.csv', header=None, \n",
    "                        names=['class', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', \n",
    "                                   'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16'])\n",
    "print(votes.shape)\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Topics\n",
    "Each of the vote columns in the above data frame corresponds to the following topics, so let's replace the column names with something easier to remember:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vote_names = [\n",
    "    'handicapped_infants',\n",
    "    'water_project_cost_sharing',\n",
    "    'adoption_of_the_budget_resolution',\n",
    "    'physician_fee_freeze',\n",
    "    'el_salvador_aid',\n",
    "    'religious_groups_in_schools',\n",
    "    'anti_satellite_test_ban',\n",
    "    'aid_to_nicaraguan_contras',\n",
    "    'mx_missile',\n",
    "    'immigration',\n",
    "    'synfuels_corporation_cutback',\n",
    "    'education_spending',\n",
    "    'superfund_right_to_sue',\n",
    "    'crime',\n",
    "    'duty_free_exports',\n",
    "    'export_administration_act_south_africa']\n",
    "\n",
    "votes.columns = ['class'] + vote_names\n",
    "votes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual exploration - Bar Plots\n",
    "To further understand this data, let's make some plots of the first 5 votes. The code in the cell below creates bar plots for these votes by `Class` or political parties. Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's also convert our dataframe columns to \"category\" type to take advantage\n",
    "# categorical utilities like facet-based plotting in seaborn\n",
    "votes['class'] = votes['class'].astype('object').astype(\n",
    "    'category', categories=['republican', 'democrat'])\n",
    "\n",
    "values = ['y', 'n', '?']\n",
    "for c in votes.columns[1:]:\n",
    "    votes[c] = votes[c].astype('object').astype(\n",
    "        'category', categories=values)\n",
    "democrats = votes[votes['class'] == 'democrat']\n",
    "republicans = votes[votes['class'] == 'republican']\n",
    "for vote_col in votes.columns[1:6]:\n",
    "    pyplot.subplot(1, 2, 1)\n",
    "    pyplot.hist([1 if x == 'y' else 0 for x in democrats[vote_col]])\n",
    "    pyplot.title('Democrats: \\n' + vote_col)\n",
    "    pyplot.ylabel('count')\n",
    "    pyplot.subplot(1, 2, 2)\n",
    "    pyplot.hist([1 if x == 'y' else 0 for x in republicans[vote_col]])\n",
    "    pyplot.title('Republicans: \\n' + vote_col)\n",
    "    pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When examining these charts, keep in mind that there are more Democrats than Republicans in this data set. What is important is the probability of a Yes vote or No vote for the members of each party. Some votes are quite skewed by party affiliation, such as 'adoption of the budget resolution'. Whereas, some votes have similar probabilities by party, such as 'water project cost sharing'. These probabilities of votes by party are used to train the naive Bayes model.\n",
    "\n",
    "### Train and Test the Model\n",
    "Now that we understand a bit about the characteristics of the data, it's time to train and test a naive Bayes model. The python `sklearn.naive_bayes` package provides a library that trains a naive Bayes model and produces a model object that can make predictions on new instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_col = 'class'\n",
    "\n",
    "# We need to convert our categorical values to numeric feature vectors\n",
    "feature_vecs = numpy.array([\n",
    "        votes[c].cat.codes \n",
    "        for c in votes.columns \n",
    "        if c != label_col]).T\n",
    "feature_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we also need to convert our label (democrat vs republican) to numeric values\n",
    "labels = votes[label_col].cat.codes\n",
    "# take a look at the mapping for the first 5 values like so\n",
    "list(zip(votes[label_col][:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.naive_bayes\n",
    "\n",
    "# Define the model\n",
    "model = sklearn.naive_bayes.MultinomialNB(alpha=1e-7)\n",
    "# Train the model with our votes dataset\n",
    "model.fit(feature_vecs, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "With the model computed, let's evaluate the performance. We can get a quick overview of the model's effectiveness by printing the first 10 rows of the result. Execute the code in the cell below to print the first 10 rows of the result and examine the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs[:10])\n",
    "party_probabilities = model.predict_proba(feature_vecs[:10])\n",
    "\n",
    "results = pandas.DataFrame({\n",
    "        'party': votes['class'][:10],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'][:10].cat.categories),\n",
    "        'proba(Republican)': party_probabilities[:, 0],\n",
    "        'proba(Democrat)': party_probabilities[:, 1],\n",
    "    })\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you examine these results notice:\n",
    "1. There is 1 classification error, with 9 cases correctly classified.\n",
    "2. In most cases, the probability of the class predicted (score) is much larger than for the other class, including for the cases with classification errors.\n",
    "3. One case has nearly identical probabilities for the classes.\n",
    "\n",
    "As a next step, we can compute the confusion matrix and performance metrics for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "\n",
    "def confusion_matrix(labels, predicted_labels, label_classes):\n",
    "    return pandas.DataFrame(\n",
    "        sklearn.metrics.confusion_matrix(labels, predicted_labels),\n",
    "        index=[label_classes], \n",
    "        columns=label_classes)    \n",
    "\n",
    "def performance(results):\n",
    "    accuracy = sklearn.metrics.accuracy_score(\n",
    "        results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "    precision = sklearn.metrics.precision_score(\n",
    "            results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "    recall = sklearn.metrics.recall_score(\n",
    "            results['party'].cat.codes, results['predicted'].cat.codes)\n",
    "\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n",
    "    \n",
    "    return confusion_matrix(\n",
    "        results['party'], \n",
    "        results['predicted'], \n",
    "        results.party.cat.categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are reasonably good looking at only the first 10 of 435 congresspeople. \n",
    "\n",
    "Execute the code in the cell below to compute and print an evaluation of the model using all the data and compare the results to the first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs)\n",
    "party_probabilities = model.predict_proba(feature_vecs)\n",
    "\n",
    "results_all = pandas.DataFrame({\n",
    "        'party': votes['class'],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'].cat.categories),\n",
    "        'proba(democrat)': party_probabilities[:, 0],\n",
    "        'proba(republican)': party_probabilities[:, 1],\n",
    "    })\n",
    "performance(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "The Laplace smoothing method is an effective way to deal with data sets which do not have sufficient samples to compute probabilities. This method avoids the case where $p(x_j|C_k) = 0$. \n",
    "\n",
    "The code in the cell below computes a naive Bayes model using the same congressional vote data, but with a Laplace smoother with a span of 3 data points. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The `alpha` param controls the Laplace smoothing\n",
    "model = sklearn.naive_bayes.MultinomialNB(alpha=3)\n",
    "model.fit(feature_vecs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_party = model.predict(feature_vecs)\n",
    "party_probabilities = model.predict_proba(feature_vecs)\n",
    "\n",
    "results_all = pandas.DataFrame({\n",
    "        'party': votes['class'],\n",
    "        'predicted': pandas.Categorical.from_codes(\n",
    "            predicted_party, votes['class'].cat.categories),\n",
    "        'proba(democrat)': party_probabilities[:, 0],\n",
    "        'proba(republican)': party_probabilities[:, 1],\n",
    "    })\n",
    "performance(results_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are similar to the model computed without Laplace smoothing. This result is expected as all the cases in the data set have sufficient data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example - Census Income\n",
    "\n",
    "Let's try another binary classification example. The code in the cell below loads some sample US Census data. We want to build and evaluate a naive Bayes model to classify people by high and low income using $50,000 as the cut-off. Execute this code and examine the features in the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income = pandas.read_csv('https://library.startlearninglabs.uw.edu/DATASCI410/Datasets/Adult%20Census%20Income%20Binary%20Classification%20dataset.csv', \n",
    "                         sep=', ', engine='python')\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing Features\n",
    "We can see some features which are likely to be collinear. There is also one feature, 'fnlwgt', which is not useful in classifying these people. The code in the cell below removes these columns. Execute this code to create a data set with reduced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "income = income.drop(['workclass', 'fnlwgt', 'education-num', 'relationship'], axis=1)\n",
    "income.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 1\n",
    "Compute a naive Bayes model to classify `income` using the features in the Income data set. Use `laplace = 3` for smoothing.  Print the model and examine the conditional probabilities for the values of the features to get an idea how the classifer works. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace ??? with the correct parameters\n",
    "\n",
    "for c in income.columns:\n",
    "    if income[c].dtype == 'object':\n",
    "        income[c] = income[c].astype(???)\n",
    "\n",
    "income_label_col = 'income'\n",
    "income_labels = income[income_label_col].cat.codes\n",
    "\n",
    "features = []\n",
    "for c in income.columns:\n",
    "    if c != income_label_col:\n",
    "        if str(income[c].dtype) == ???:\n",
    "            features.append(income[c].cat.codes)\n",
    "        else:\n",
    "            features.append(income[c])\n",
    "income_feature_vecs = numpy.array(features).T\n",
    "\n",
    "# Create the Model\n",
    "model = sklearn.naive_bayes.MultinomialNB(???)\n",
    "model.fit(income_feature_vecs, income_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conditional_probas = pandas.DataFrame(model.feature_log_prob_, columns=income.columns[:-1])\n",
    "conditional_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(-conditional_probas).plot(kind=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 2\n",
    "Using the model you computed for predicting peoples' income class, compute predictions (scores). Also compute and print the performance metrics. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace ??? with the correct parameters\n",
    "predicted_income = model.predict(???)\n",
    "income_probabilities = model.predict_proba(???)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(income_labels, predicted_income)\n",
    "precision = sklearn.metrics.precision_score(income_labels, predicted_income)\n",
    "recall = sklearn.metrics.recall_score(income_labels, predicted_income)\n",
    "print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n",
    "print('Confusion matrix:')\n",
    "confusion_matrix(income_labels, predicted_income, \n",
    "                 income[income_label_col].cat.categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Number of Features\n",
    "\n",
    "Let's investigate the effect of adding more data samples to the naive Bayes model. The code in the cell below computes and evaluates naive Bayes models using 2, 3, 4, 5 and 6 votes. Execute this code and compare the results to those obtained using the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_col = 'class'\n",
    "\n",
    "# We need to convert our categorical values to numeric feature vectors\n",
    "feature_vecs = numpy.array([\n",
    "        votes[c].cat.codes \n",
    "        for c in votes.columns \n",
    "        if c != label_col]).T\n",
    "print(feature_vecs.shape)\n",
    "\n",
    "# we also need to convert our label (democrat vs republican) to numeric values\n",
    "labels = votes[label_col].cat.codes\n",
    "# take a look at the mapping for the first 5 values like so\n",
    "list(zip(votes[label_col][:5], labels[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num_features in [2, 3, 4, 5, 6]:\n",
    "    model = sklearn.naive_bayes.MultinomialNB()\n",
    "    model.fit(feature_vecs[:, :num_features], labels)\n",
    "\n",
    "    predicted_votes = model.predict(feature_vecs[:, :num_features])\n",
    "\n",
    "    print('Number of features = %d' % num_features)\n",
    "    accuracy = sklearn.metrics.accuracy_score(labels, predicted_votes)\n",
    "    precision = sklearn.metrics.precision_score(labels, predicted_votes)\n",
    "    recall = sklearn.metrics.recall_score(labels, predicted_votes)\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(labels, predicted_votes, votes['class'].cat.categories))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from these results, that the model gains accuracy rapidly with just a few features. In fact, 5 or 6 of the 16 features provides equivalent results. \n",
    "\n",
    "***\n",
    "## Your Turn 3\n",
    "Compute a naive Bayes model, compute and  print the performance statistics for 100, 500, 1000, 2000, 8000, and 32561 rows of the Income data set. How many rows are required until the performance is close to the best possible with the model?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace ??? with the Correct Parameters\n",
    "\n",
    "for num_rows in (???):\n",
    "    model = sklearn.naive_bayes.MultinomialNB()\n",
    "    model.fit(income_feature_vecs[:num_rows, :], income_labels[:num_rows])\n",
    "\n",
    "    predicted_income = model.predict(income_feature_vecs[:num_rows, :])\n",
    "    income_probabilities = model.predict_proba(income_feature_vecs[:num_rows, :])\n",
    "    \n",
    "    accuracy = sklearn.metrics.accuracy_score(income_labels[:num_rows], predicted_income)\n",
    "    precision = sklearn.metrics.precision_score(income_labels[:num_rows], predicted_income)\n",
    "    recall = sklearn.metrics.recall_score(income_labels[:num_rows], predicted_income)\n",
    "    print('Accuracy = %.3f, Precision = %.3f, Recall = %.3f' % (accuracy, precision, recall))\n",
    "    print('Confusion matrix:')\n",
    "    print(confusion_matrix(\n",
    "            income_labels[:num_rows], predicted_income, income.income.cat.categories), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have worked through the following:\n",
    "\n",
    "1. Theory of naive Bayes models\n",
    "2. Pitfalls of naive Bayes \n",
    "3. Good regularization properties \n",
    "4. Computationally efficiency \n",
    "5. Examples of computing and evaluating naive Bayes models\n",
    "6. Examine the effects of data set size on the results of naive Bayes models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
