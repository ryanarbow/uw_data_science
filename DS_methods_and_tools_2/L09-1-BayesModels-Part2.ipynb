{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"instructions\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h1>Instructions</h1>\n",
    "<p>Look for the <b>5 Your Turn</b> sections to complete the code and/or answer questions.<p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 9 - Bayesian Modeling and Markov Chain Monte Carlo\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook introduces you to a general and flexible form of Bayesian modeling using the **Markov chain Monte Carlo** methods. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Note:** To run this notebook you must have the following packages installed:\n",
    "- pymc3\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Uncomment the following line to install pymc3\n",
    "# !pip install pymc3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Review of Bayes Theorem\n",
    "\n",
    "Recall Bayes theorem:\n",
    "\n",
    "$$P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Computing the normalization $P(B)$ is a bit of a mess. But fortunately, we don't always need the denominator. We can rewrite Bayes Theorem as:\n",
    "\n",
    "$$𝑃(𝐴│𝐵)=𝑘∙𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "Ignoring the normalizaton constant $k$, we get:\n",
    "\n",
    "$$𝑃(𝐴│𝐵) \\propto 𝑃(𝐵|𝐴)𝑃(𝐴)$$\n",
    "\n",
    "### Bayesian Parameter Estimation\n",
    "\n",
    "How to we interpret the relationships shown above? We do this as follows:\n",
    "\n",
    "$$Posterior\\ Distribution \\propto Likelihood \\bullet Prior\\ Distribution \\\\\n",
    "Or\\\\\n",
    "𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠│𝑑𝑎𝑡𝑎) \\propto 𝑃(𝑑𝑎𝑡𝑎|𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠)𝑃(𝑝𝑎𝑟𝑎𝑚𝑒𝑡𝑒𝑟𝑠) $$\n",
    "\n",
    "These relationships apply to the observed data distributions, or to parameters in a model (partial slopes, intercept, error distributions, lasso constant,…). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequentist vs. Bayesian Models\n",
    "\n",
    "Let's summarize the differences between the Bayesian and Frequentist views. \n",
    "\n",
    "- Bayesian methods use priors to quantify what we know about parameters.\n",
    "- Frequentists do not quantify anything about the parameters, using p-values and confidence intervals to express the unknowns about parameters.\n",
    "\n",
    "Accepting that both views are useful, we can contrast these methods with a chart.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/FrequentistBayes.jpg\" style=\"height: 500px;\"  alt=\"Frequentist versus Bayes\" title=\"Frequentist versus Bayes\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Sampling and Scalability\n",
    "\n",
    "Real-world Bayes models have large numbers of parameters, even into the millions. A naive approach to Bayesian analysis would be to simply grid sample across the dimensions of the parameter space. However, grid sampling will not scale. To understand the scaling problem, do the following thought experiment, where each dimension is sampled 100 times:\n",
    "\n",
    "- For a 1-parameter model: $100$ samples.\n",
    "- For a 2-parameter model: $100^2 = 10000$ samples.\n",
    "- For a 3-parameter model: $100^3 = 10^5$ samples.\n",
    "- For a 100-parameter model: $100^{100} = 10^{102}$ samples. \n",
    "\n",
    "As you can see, the computational complexity of grid sampling has **exponential scaling** with dimensionality. Clearly, we need a better approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Markov Chain Monte Carlo\n",
    "\n",
    "Large-scale Bayesian models use a family of efficient sampling methods known as **Markov chain Monte Carlo sampling**. MCMC methods are computationally efficient, but it requires some effort to understand how they work and  what to do when things go wrong. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### What is a Markov Process?\n",
    "\n",
    "As you might guess, a MCMC sampling uses a chain of **Markov sampling processes**. The chain is built from a sequence of individual Markov processes. A Markov process is any process that a makes a transition from one state other states with probability $\\Pi$ with **no dependency on past states**. In summary, a Markov process has the  following properties:\n",
    "- $\\Pi$  only depends on the current state\n",
    "- Transition from current state to one or more other states\n",
    "- Can ‘transition’ to current state\n",
    "- A matrix $\\Pi$ of dim N X N for N possible state transitions\n",
    "- A Markov process is a random walk since any possible transition to a new state, $j$, can occur from each state, $i$, if $p_{ij} \\gt 0$.\n",
    "\n",
    "Since a Markov chain is a **memoryless** sequence of Markov transition processes, we can write:\n",
    "\n",
    "$$P(X_{t + 1}| X_t = x_t, \\ldots, x_0 = x_t) = p(X_{t + 1}| x_t)$$\n",
    "\n",
    "Since the Markov process is memoryless, the transition probability only depends on the current state, not any previous states. \n",
    "\n",
    "For a system with $N$ possible states we can write the transition matrix $\\Pi$ for the probability of transition from one state to another:\n",
    "\n",
    "$$\\Pi = \n",
    "\\begin{bmatrix}\n",
    "\\pi_{1,1} & \\pi_{1,2} & \\cdots & \\pi_{1, N}\\\\\n",
    "\\pi_{2,1} & \\pi_{2,2} & \\cdots & \\pi_{2,N}\\\\\n",
    "\\cdots & \\cdots & \\cdots & \\cdots \\\\\n",
    "\\pi_{N,i} & \\pi_{N,2} & \\cdots & \\pi_{N,N}\n",
    "\\end{bmatrix}\\\\\n",
    "where\\\\\n",
    "\\pi_{i,j} = probability\\ of\\ transition\\ from\\ state\\ i\\ to\\ state\\ j\\\\\n",
    "and\\\\\n",
    "\\pi_{i,i} = probability\\ of\\ staying\\ in\\ state\\ i\\\\\n",
    "further\\\\\n",
    "\\pi_{i,j} \\ne \\pi_{j,i}\\ in\\ general\n",
    "$$\n",
    "\n",
    "Notice that none of these probabilities depend on the previous state history."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCMC and the Metropolis Algorithm\n",
    "\n",
    "The first MCMC sampling algorithm developed is the **Metropolis-Hastings algorithm** (Metropolis et al. (1953), Hastings (1970)). This algorithm is often referred to as the Metropolis algorithm. The Metropolis-Hastings algorithm has the following steps to estimate the density of the likelihood of the parameters:\n",
    "\n",
    "1. Pick a starting point in your parameter space and evaluate the posterior according to your model. In other words, take an initial sample of the likelihood $p(data|parameters)$.\n",
    "\n",
    "2. Choose a nearby point in parameter space randomly and evaluate the likelihood at this point. A probability distribution is used to make this random selection. The normal distribution is a common choice.\n",
    "  - If the $p(data | parameters)$ of the new point is greater than your current point, accept new point and move there.\n",
    "  - If the $p(data | parameters)$ of the new point is less than your current point, only accept with probability according to the ratio:  \n",
    "$$Acceptance\\ probability\\ = \\frac{p(data | new\\ parameters)}{p(data | previous\\ parameters)}$$\n",
    "\n",
    "3. Repeat step 2 many times.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have outlined the basic Metropolis-Hastings MCMC algorithm, let's examine some of its properties.\n",
    "\n",
    "- Since the Metropolis-Hastings algorithm samples the parameter space we only need to visit a limited number of points, rather than sample an entire grid. \n",
    "- The Metropolis-Hastings algorithm is guaranteed to **eventually converge** to the underlying distribution.\n",
    "- If there is high serial correlation from one sample to the next in Metropolis-Hastings chain converges slowly. \n",
    "- To ensure efficient convergence we Need to ‘tune’ the state selection probability distribution used to find the next point. For example if we use normal distribution we must pick $\\sigma$. If $\\sigma$ is too small, the chain will only search the space slowly, with small jumps. If $\\sigma$ is too big, there are large jumps which slow convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings Algorithm Example\n",
    "\n",
    "Let's make these concepts concrete, by trying a simple example.\n",
    "\n",
    "As a first step, lets plot a set of points with density determined by the a bi-variate normal distribution. Execute the code below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3\n",
    "\n",
    "import pandas\n",
    "import numpy\n",
    "import seaborn\n",
    "from matplotlib import pyplot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = numpy.array([[1, .6], [.6, 1]])\n",
    "mu = numpy.array([.5, .5])\n",
    "random_points = numpy.random.multivariate_normal(mean=mu, cov=sigma,  size=10000)\n",
    "pyplot.scatter(random_points[:, 0], random_points[:, 1], alpha=.1)\n",
    "pyplot.xlabel('X')\n",
    "pyplot.ylabel('Y')\n",
    "pyplot.title('Draws from a bivariate normal distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This plot looks as expected. The density of the dots is proportional to the probabilities. You can see the effect of the covariance structure in these data.\n",
    "\n",
    "As a next step, let's look at the density of the marginal probabilities of the $X$ and $Y$ variables. The code in the cell below plots histogram and density plots of the marginals. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.hist(random_points[:, 0], normed=True, bins=40)\n",
    "pyplot.title('Marginal X distribution')\n",
    "pyplot.xlabel('X')\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.hist(random_points[:, 1], normed=True, bins=40)\n",
    "pyplot.title('Marginal Y distribution')\n",
    "pyplot.xlabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that these distributions are approximately normal, but with a right skew. \n",
    "\n",
    "Now, we are ready to sample these data using the Metropolis-Hastings MCMC algorithm. The code in the cell below performs the following operations:\n",
    "\n",
    "1. Compute the likelihood of the bi-variate normal distribution. \n",
    "2. Initialize the chain.\n",
    "3. Initialize some performance statistics.\n",
    "4. Sample the likelihood of the data using the Metropolis-Hastings algorithm.\n",
    "5. Plot the result.\n",
    "\n",
    "Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [0, 0]\n",
    "\n",
    "# Calculate the likelihood of a vector `x` for a multivariate normal\n",
    "# distribution MVN(mu, sigma)\n",
    "def likelihood(x, mu, sigma):\n",
    "    x = numpy.asarray(x)\n",
    "    numerator = numpy.exp(-numpy.matrix(x-mu)\n",
    "                  * numpy.linalg.inv(sigma)\n",
    "                  * numpy.matrix(x-mu).T/2)\n",
    "    denominator = (numpy.pi * numpy.sqrt(numpy.linalg.det(sigma) / 2))\n",
    "    return numpy.asscalar(numerator) / denominator\n",
    "\n",
    "# where to start\n",
    "chain = [[4, -4]]\n",
    "chain_length = 10000\n",
    "\n",
    "# Evaluate the current position\n",
    "current_likelihood = likelihood(chain[0], mu, sigma)\n",
    "\n",
    "# Keep track of how often we accept or reject a proposal:\n",
    "accept_count = 0\n",
    "reject_count = 0\n",
    "\n",
    "for i in range(chain_length-1): # chain length minus 1 because we already have a point (the starting point)\n",
    "    # Sample the direction of the move we'll propose\n",
    "    delta = numpy.random.multivariate_normal([0, 0], numpy.diag([.1, .1]))\n",
    "    # Our new proposal point is our previous position plus the sampled move\n",
    "    proposed = chain[-1] + delta\n",
    "    proposed_likelihood = likelihood(proposed, mu, sigma)\n",
    "    \n",
    "    # Accept according to probability\n",
    "    if (numpy.random.uniform() < (proposed_likelihood / current_likelihood)):\n",
    "        accept_count += 1\n",
    "        current_likelihood = proposed_likelihood\n",
    "        chain.append(proposed)\n",
    "    else:\n",
    "        chain.append(chain[-1])\n",
    "        reject_count += 1\n",
    "    \n",
    "chain = numpy.asarray(chain)\n",
    "pyplot.scatter(chain[:, 0], chain[:, 1], alpha=.25)\n",
    "_ = pyplot.title('MCMC values for bivariate normal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the long 'tail' on the sampled distribution. This behavior arises from the initial wandering of the Markov chain as it finds the high probability regions of the distribution. This period in which the Markov chain wanders is known as the **burn-in period**.\n",
    "\n",
    "The code in the cell below, plots the same Markov chain, but with the first 1000 values removed. The remaining samples are from the post burn-in chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_burnin = round(.1 * chain_length)\n",
    "\n",
    "pyplot.scatter(chain[num_burnin:, 0], chain[num_burnin:, 1], alpha=.25)\n",
    "_ = pyplot.title('MCMC values for bivariate normal with burn-in')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In the plot above you can see that there is no 'tail' in the sampled distribution. As expected, the tail was sampled during the burn-in period and was not significant in sampling the distribution.\n",
    "\n",
    "Let's plot the density of the marginal distribution of these samples. We can then compare these densities to those of the original data we generated. The code in the cell below plots a histogram of the sampled marginal distributions along with the density of the original samples. Execute this code and compare the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.hist(chain[num_burnin:, 0], normed=True, bins=40)\n",
    "pyplot.title('Marginal X distribution')\n",
    "pyplot.xlabel('X')\n",
    "pyplot.show()\n",
    "\n",
    "pyplot.hist(chain[num_burnin:, 1], normed=True, bins=40)\n",
    "pyplot.title('Marginal Y distribution')\n",
    "_ = pyplot.xlabel('Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that the histograms of the sampled marginal distributions are close to the marginal density of the original data. There is some skew as a result of sampling error.\n",
    "\n",
    "Next, lets compare the **Maximum a posteriori or MAP** point of the sampled marginal distributions to the original means for $x$ and $y$. The code in the cell below approximates the MAP using the `mean` function. Execute this code and compare the results to the original data with $x = 0.5$ and $y = 0.5$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mcmc_map = numpy.mean(chain, axis=0)\n",
    "mcmc_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The MAP values of the sampled marginal distributions are close to the values for the original data. However, the mean approximation for the MAP seems to be biased by the skew in the sampled distribution.\n",
    "\n",
    "Let's turn our attention to the convergence properties of the Metropolis-Hastings MCMC sampler. The **acceptance rate** and **rejection rate** are key convergence statistics for the Metropolis-Hastings algorithm. A low acceptance rate and high rejection rate are signs of poor convergence. Execute the code in the cell below which computes and displays these statistics and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Acceptance rate = %.2f' % (accept_count / chain_length))\n",
    "print('Rejection rate = %.2f' % (reject_count / chain_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "These statistics indicate good convergence with a fairly low rejection rate.\n",
    "\n",
    "Another way to evaluate the convergence of MCMC algorithms is to look at the **trace** of the samples. The trace is a plot of the sample value with sample number. The code in the cell below plots the trace for both the $x$ and $y$ samples, including the burn-in period. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12, 3))\n",
    "pyplot.plot(chain[:, 0])\n",
    "pyplot.title('X chain')\n",
    "\n",
    "pyplot.figure(figsize=(12, 3))\n",
    "pyplot.plot(chain[:, 1])\n",
    "pyplot.title('Y chain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Examine these sample traces. Notice that there is a significant excursion during the initial burn-in period. After the initial burn-in you can see that the sampling wanders around the mode of the distribution, as it should. \n",
    "\n",
    "Let's look at a close-up view the portion of these traces just after the burn-in period. The code in the cell below plots samples 1000 to 2000. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Look at a shorter segment of the chain\n",
    "# Always look at the chain, we would like random noise centered around means\n",
    "pyplot.figure(figsize=(12, 3))\n",
    "pyplot.plot(chain[1000:2000, 0])\n",
    "pyplot.title('X chain')\n",
    "\n",
    "pyplot.figure(figsize=(12, 3))\n",
    "pyplot.plot(chain[1000:2000, 1])\n",
    "pyplot.title('Y chain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that, for the most part, the samples are centered on the MAP for $x$ and $y$. This is the ideal behavior of the Metropolis-Hastings algorithm. \n",
    "\n",
    "Finally, let's take a look at the autocorrelation of our samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(12, 4))\n",
    "pyplot.subplot(1, 2, 1)\n",
    "pyplot.title('Autocorrelation of X')\n",
    "pandas.plotting.autocorrelation_plot(chain[:, 0], ax=pyplot.gca())\n",
    "\n",
    "pyplot.subplot(1, 2, 2)\n",
    "pyplot.title('Autocorrelation of Y')\n",
    "pandas.plotting.autocorrelation_plot(chain[:, 1], ax=pyplot.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Gibbs Sampling and Hierarchical Models\n",
    "\n",
    "With some experience with the Metropolis-Hastings MCMC algorithm, let's try a Bayes hierarchical model example using Gibbs sampled MCMC. Hierarchical models can be quite complex and provide a great deal of flexibility. The Gibbs sampler can provide a significant improvement in efficiency over the Metropolis-Hastings algorithm. \n",
    "\n",
    "### Gibbs Sampling\n",
    "\n",
    "The Metropolis-Hastings algorithm is a useful tool. However, this algorithm can suffer from slow convergence for several reasons:\n",
    "\n",
    "- Samples from the Metropolis-Hastings algorithm generally have a fairly high serial correlation. This problem results from taking steps in random directions.\n",
    "- As already mentioned, we need to ‘tune’ the state selection probability distribution used to find the next point. For example if we use normal distribution we must pick $\\sigma$. If $\\sigma$ is too small, the chain will only search the space slowly, with small jumps. If $\\sigma$ is too big, there are large jumps which slow convergence.\n",
    "\n",
    "The Gibbs sampler (Geman and Geman, 1984) is an improved MCMC sampler which speeds convergence. The basic Gibbs sampler algorithm has the following steps:\n",
    "\n",
    "1. For an N dimensional parameter space, $\\{ \\theta_1, \\theta_2, \\ldots, \\theta_N \\}$, find a random starting point. \n",
    "2. Starting with dimension $1$, cycle through each dimension in order, $\\{1, 2, 3, \\ldots, N\\}$:  \n",
    "  - Sample the marginal distribution of the parameter based on the probability distribution of the parameter given the data and other parameter values:\n",
    "  $$p(\\theta_1|D, \\theta_2, \\theta_3, \\ldots, \\theta_N)\\\\ \n",
    "  where\\\\\n",
    "  D\\ is\\ the\\ data$$\n",
    "  - Repeat this sampling procedure for each remaining dimension in order, $\\{2, 3, \\ldots, N\\}$.\n",
    "3. Repeat step 2 until convergence.    \n",
    "\n",
    "From this simplified description of the Gibbs sampling algorithm you can infer:\n",
    "\n",
    "- When compared to the Metropolis-Hastings algorithm, the Gibbs sampler reduces serial correlation owing to the reduced round-robin nature of the sampling.   \n",
    "- There are no tuning parameters since sampling is based on the marginals of the likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Modeling Example\n",
    "\n",
    "In this case, we will under take a univariate regression problem using synthetic data. The regression model has two parameters a slope and an intercept. The variance of the data is an additional 'nuisance' parameter. To compute these parameters, accounting for their dependency, we will use a hierarchical Bayes model. \n",
    "\n",
    "Hierarchical Bayes models depend on the **chain rule** for Bayes theorem. The chain rule allows us to expand Bayes theorem to accommodate multi-parameter models. We can write the basic chain rule for Bayes theorem like this:\n",
    "\n",
    "$$p(\\theta, \\sigma | D) \\propto p(D| \\theta, \\sigma) p(\\theta, \\sigma)\\\\\n",
    "\\propto p(D | \\theta) p(\\theta | \\sigma) p(\\sigma)\\\\\n",
    "\\propto\\ Likelihood\\ *\\ Prior\\ of\\ \\theta\\ given\\ \\sigma\\ *\\ Prior\\ of\\ \\sigma$$\n",
    "\n",
    "As you can see, a complex multi-parameter Bayesian model is transformed to a hierarchy. The hierarchy is a chain of prior distributions (unconditional and conditional) and a likelihood dependent only on one parameter.  \n",
    "\n",
    "As a first step the code in the cell below generates bi-variate data with normally distributed errors and plots the result. Execute this code to compute the data.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Set up the data set as a regression problem\n",
    "N = 1000\n",
    "x = numpy.arange(N)\n",
    "epsilon = numpy.random.normal(0, 100, N)\n",
    "y = x + epsilon\n",
    "\n",
    "pyplot.scatter(x, y, alpha=.5)\n",
    "_ = pyplot.title('Synthetic data for Bayes regression problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model has two parameters, a slope and an intercept, which we will call $a$ and $b$. We will use a **hierarchical Bayes model**. The model is considered hierarchical since the quantity we really want to know, the posterior distribution of the label, which we will refer to as $\\hat{y}$, depends on the distribution of other model parameters. In this case, the posterior distribution of $\\hat{y}$ depends on both the regression coefficients and an error term. We can visualize the hierarchical relationships in this model in the diagram below.\n",
    "\n",
    "<img src=\"https://library.startlearninglabs.uw.edu/DATASCI410/img/HierarchicalModel.jpg\" alt=\"Hierarchical model for the posterior distribution of y\" title=\"Hierarchical model for the posterior distribution of y\" style=\"height: 500px;\">\n",
    "\n",
    "In mathematical terms we can define the hierarchical model as follows:\n",
    " \n",
    "1. The prior of the dispersion, $\\sigma$, of the power distribution is defined as the uniform distribution:\n",
    "$$U(0, 100)$$\n",
    "2. The variance (dispersion) of the label values is modeled as an power distribution:\n",
    "$$\\tau = a x^\\sigma = -2 x^\\sigma$$\n",
    "3. The prior distributions of the regression model, $a$ and $b$, are modeled as normal distributions:\n",
    "$$N(0, 0.01)$$\n",
    "4. The regression model for estimating $\\hat{y_i}$ is defined by:\n",
    "$$\\hat{y_i} = a + b x_i$$\n",
    "5. The posterior distribution of the label values is modeled as a normal distribution:\n",
    "$$N(\\hat{y_i}, \\tau)$$\n",
    "\n",
    "### Computing the Model with pyMC3\n",
    "\n",
    "To compute the MCMC samples we will use the `pymc3` package\n",
    "\n",
    "Each of the variables in the Bayesian model diagram above are represented by pyMC3 objects in the model definition below. The model combines our data (`x` and `y`) with the variables we'd like to estimate: our intercept `a` and slope `b`\n",
    "\n",
    "Internally, MCMC uses the No U-Turn Sampler (NUTS) for simulation. Instead of proposing new samples with the Metropolis-Hastings acceptance criteria or exploring using the Gibbs sampler approach as described above, NUTS models the exploration as the movement of a particle through a field. The field that guides the movement of the particle through the space is derived from the target probability distribution, such that the particle is drawn towards dense (high likelihood) regions of the space. This strategy directs the exploration of the space rather than using a random wandering behavior as we saw with MCMC methods above, and in doing so is typically more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(x, y):\n",
    "    model = pymc3.Model()\n",
    "\n",
    "    with model:\n",
    "        a = pymc3.Normal('a', mu=0, sd=.01)\n",
    "        b = pymc3.Normal('b', mu=0, sd=.01)\n",
    "        \n",
    "        # We model the y variable (y_hat) as a linear function of our input \n",
    "        # array x, having intercept=a and slope=b.\n",
    "        y_hat = a + b * x\n",
    "\n",
    "        sigma = pymc3.Uniform('sigma', lower=0, upper=100)\n",
    "        tau = pow(sigma, -2)\n",
    "\n",
    "        # Here we assign our y array as the observed values of the model.\n",
    "        y_obs = pymc3.Normal('y_obs', mu=y_hat, tau=tau, observed=y)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 1\n",
    "Use a reduced size data set of 50 samples to compute another Bayesian regression model. Ensure you do the following:\n",
    "1. Use the `numpy.random.choice` function to create an index for the samples of `x` and `y`.\n",
    "2. You may wish to plot your sampled data to ensure your sampling worked as desired.\n",
    "3. Define a new model with the sampled dataset.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameters\n",
    "\n",
    "index = numpy.random.choice(numpy.arange(N), size=???)\n",
    "\n",
    "x_50 = x[index]\n",
    "y_50 = y[index]\n",
    "\n",
    "pyplot.scatter(???, ???, alpha=.5)\n",
    "_ = pyplot.title('Synthetic data for Bayes regression problem for 50x sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameters\n",
    "\n",
    "# Create a new model\n",
    "model_50 = create_model(???, ???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "With the model compiled and the posterior sampled, we can now extract the samples. The `pymc3.sample` function extracts the samples from our Markov chain. Execute this code to extract the samples. \n",
    "\n",
    "**Please Note**: This code will take some time to run through 1000 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_chains = 4\n",
    "\n",
    "## Compute some samples\n",
    "with model:\n",
    "    samples = pymc3.sample(1000, chains=num_chains)\n",
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Your Turn 2\n",
    "Extract samples for the model you created with 50 data points. Make sure you give another name to these samples. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameters\n",
    "\n",
    "# Compute samples for the Model with 50 data points\n",
    "with ???:\n",
    "    samples_50 = pymc3.sample(1000, chains=???)\n",
    "samples_50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now examine the convergence properties of the MCMC. As a first step, the code below plots the traces of chains (4) and the marginal density of the slope and intercept parameters, $a$ and $b$. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = pymc3.traceplot(samples, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine these plots, noting the following:\n",
    "\n",
    "1. The trace plots on the right show the path of the 4 MCMC chains for the $a$ and $b$ model parameters.\n",
    "2. The density for the $a$ and $b$ model parameters are shown on the left. The MAP value of the intercept (`a`) is close to the actual value of  0.0, and the MAP value of the slope is close to the actual value of 1.0 (`b`).\n",
    "\n",
    "***\n",
    "## Your Turn 3 \n",
    "Plot the samples you extracted from the model computed with 50 data points. How do these results compare the model computed with 1000 data points?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameter\n",
    "\n",
    "# Plots of Model with 50 data points\n",
    "_ = pymc3.traceplot(???, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics\n",
    "pyMC3 can also print summary statistics for the MCMC sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary of Model with 1000 data points\n",
    "pymc3.summary(samples, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary shows a lot of useful information, including:\n",
    "    \n",
    "1. The mean of the coefficient values. You can see that the intercept is close to zero and the slope is close to 1\n",
    "2. The standard deviation (`sd`) of the coefficient values. In this case you can see that the intercept, $a$, is close to 0, as well as the slope, $b$.\n",
    "3. The sampling error (`mc_error`) is the error arising from the MCMC sampling.\n",
    "4. The quantiles for the model parameters (`hpd_2.5` and `hpd_97.5`)\n",
    "5. The effective number of samples (`n_eff`) out of `n_chain` * `n_samples` possible samples\n",
    "6. The Gelman-Rudin statistic (`Rhat`) which is explained below\n",
    "\n",
    "***\n",
    "## Your Turn 4\n",
    "Display the summary of the model you created with 50 data points. Compare the results to the model computed using 1000 data points.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace ??? with the Correct Parameter\n",
    "\n",
    "# Summary of Model with 50 Data Points\n",
    "pymc3.summary(???, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Model\n",
    "Let's compare the results from MCMC samples with a conventional linear model. The code in the cell below computes a linear model and prints the summary. Execute this code and compare the results to the MCMC sample results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Linear Model of 1000 data points\n",
    "import sklearn.linear_model\n",
    "\n",
    "\n",
    "linear_model = sklearn.linear_model.LinearRegression()\n",
    "linear_model.fit(x.reshape((-1, 1)), y)\n",
    "\n",
    "print('Linear model parameters: y ~ (a=%.3f) + (b=%.3f)*x' \n",
    "      % (linear_model.intercept_, linear_model.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values and error estimates for the intercept and  slope parameters from the conventional linear model are close to those obtained with MCMC sampling. \n",
    "\n",
    "The parameters of the linear model match the true intercept and slope closely, but the\n",
    " linear model provides no information on the  posterior distribution beyond these simple metrics. For example, there are no quantiles for the coefficients. \n",
    "\n",
    "***\n",
    "## Your Turn 5\n",
    "Compute and print the summary of a linear model for the 50 data points you used for your Bayesian model. Compare the results to the Bayesian model and the linear model computed using 1000 data points.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace the ??? with the correct parameters\n",
    "\n",
    "# Linear Model of 50 data points\n",
    "linear_model_50 = sklearn.linear_model.LinearRegression()\n",
    "linear_model_50.fit(???.reshape((-1, 1)), ???)\n",
    "\n",
    "print('Linear model parameters: y ~ (a=%.3f) + (b=%.3f)*x' \n",
    "      % (linear_model_50.intercept_, linear_model_50.coef_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative Values Plot\n",
    "Another way to examine the convergence of a Markov is to plot the cumulative values of the coefficients and their standard deviation vs. the sample number. The `cumuplot` function creates just such a plot for a coda Markov chain. Execute the code in the cell below and examine the results for each chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "\n",
    "def trace_variable(samples, varnames, quantiles=(0.025, 0.5, 0.975)):\n",
    "    for i, chain in enumerate(samples.chains):\n",
    "        for j, varname in enumerate(varnames):\n",
    "            pyplot.subplot(len(samples.chains), len(varnames), i * len(varnames) + j + 1)\n",
    "            values = pandas.Series(samples.get_values(varname, chains=chain))\n",
    "            for q in quantiles:\n",
    "                pyplot.plot(\n",
    "                    values.expanding(min_periods=1).quantile(q), \n",
    "                    label='%.3f' % q)\n",
    "            pyplot.title('var=%s, chain=%d' % (varname, chain))\n",
    "            pyplot.legend(loc='upper right')\n",
    "    pyplot.tight_layout()\n",
    "\n",
    "\n",
    "pyplot.figure(figsize=(8, 12))\n",
    "trace_variable(samples, ['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots show that each chain converges to similar values and with similar standard deviations. This indicates that the Markov chains are converging properly.\n",
    "\n",
    "### Gelman-Rudin Statistic\n",
    "The **Gelman-Rudin statistic** (Gelman and Rudin, 1992) measures the ratio of the **variance shrinkage between chains** to the **variance shrinkage within chains**. The Gelman-Rudin statistic should converge to 1.0. The code in the cell below uses the `pymc3.forestplot` function to produce a plot of the Gelman-Rudin statistic and its 95% credible interval. Execute this code and examine the result (R-hat is the Gelman-Rudin statistic). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pymc3.forestplot(samples, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the value of the Gelman-Rudin statistic for our variables like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pymc3.diagnostics.gelman_rubin(samples, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gelman-Rudin statistic values for both model parameters converged to near 1.0.\n",
    "\n",
    "As we already discussed, the convergence of MCMC algorithms is slowed by **autocorrelation** between the samples. `pyMC3` provides the `autocorrplot()` function for examining the autocorrelation in Markov chains. The function creates autocorrelation function plots for each parameter and chain combination. \n",
    "\n",
    "To examine the results for the Markov chains, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = pymc3.autocorrplot(samples, varnames=['a', 'b'], max_lag=25, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective Sample Size\n",
    "You can see that there are several significant lag values of the ACF. This means that the convergence of the Markov chains was impeded by this autocorrelation.\n",
    "\n",
    "Given the significant autocorrelation in the samples, we can compute an **effective sample size or ESS**. If there is significant autocorrelation the ESS will be significantly less than the raw sample size. We can compute the ESS as follows:\n",
    "\n",
    "$$ESS = \\frac{N}{1 + 2 \\sum_k ACF(k)}$$\n",
    "\n",
    "The code in the cell below computes the ESS and rejection rate for the Markov chain. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## What is the effective size of the sample?\n",
    "pymc3.diagnostics.effective_n(samples, varnames=['a', 'b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the effective number of samples is also provided as part of the standard trace summary (see column `n_eff`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pymc3.summary(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can see that the effective sample size is much lower than the raw sample size. Still, the effective sample sizes appears to be sufficient to provide good estimates of the posterior distributions of the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook you have done the following:\n",
    "\n",
    "- Reviewed the basic properties of a Markov process\n",
    "- Performed a simple Markov chain Monte Carlo using the Metropolis-Hastings algorithm\n",
    "- Created and computed a hierarchical Bayes model using Gibbs sampling\n",
    "- Evaluated the convergence of the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div id=\"reminder\" style=\"border-radius: 5px; background-color:#f5f5f5;\" >\n",
    "<h3>Reminder</h3>\n",
    "<p>Use this notebook to answer the quiz questions related to the <b>Your Turn</b> sections.<p>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
